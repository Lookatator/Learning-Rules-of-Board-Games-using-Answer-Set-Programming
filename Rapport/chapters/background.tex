\chapter{Background}

\section{Answer Set Programming (ASP)}

\subsection{Logic Program}

We will work on finite Logic Programs with rules of the form:
\begin{itemize}
\item $\leftarrow b_1, b_2, ..., b_m, \text{ not } c_1, \text{ not } c_2,...,\text{ not } c_n$ (also called \textit{constraint}).
\item $a \leftarrow b_1, b_2, ..., b_m, \text{ not } c_1, \text{ not } c_2,...,\text{ not } c_n$. This is a \textit{default} rule.
\item $l\{a_1, a_2, ..., a_p\}u \leftarrow b_1, b_2, ..., b_m, \text{ not } c_1, \text{ not } c_2,...,\text{ not } c_n$, where $l$ and $u$ are numbers such that $l \leq u$. Also, $l\{a_1, a_2, ..., a_p\}u$ is true in an Herbrand Interpretation $I$ iff $ l \leq |\{a_1, a_2, ..., a_p\}\cap I | \leq u$. This type of rule is also called \textit{choice rule}.
\end{itemize}
In all these rules, "$\text{ not }$" refers to the \textit{"negation as failure"}, and $a, a_1, a_2, ..., a_p, b_1, b_2,\\ ..., b_m, c_1, c_2, ...,  c_n$ are atoms. 

\smallskip

$a$ and $l\{a_1, a_2, ..., a_p\}u$ are called the \textit{head} of the rule, and $b_1, b_2, ..., b_m, \text{ not } c_1, \text{ not } c_2,\\..., \text{ not } c_n$ is the \textit{body} of the rule. If we call $r$ the following rule: $\alpha \leftarrow b_1, b_2, ..., b_m,\\ \text{ not } c_1, \text{ not } c_2,...,\text{ not } c_n$ where $\alpha$ is either an atom or an aggregate, then $body^+(r)=\{b_1, b_2,...,b_m \}$ and $body^-(r)=\{c_1, c_2,...,c_n \}$.

\smallskip

A \textit{literal} is an atom or the negation (by failure) of an atom.

\subsection{Stable Models (Answer Sets)}

Let $P$ be a logic program and $X$ be an Herbrand Interpretation of $P$. From $P$ and $X$, we can construct a new program $P^X$ called the \textit{reduct} of $P$ by applying these methods:
\begin{itemize}
\item For every rule $r\in P$, if $X\cap body^-(r)=\emptyset$, then we remove every negative literal in $r$.
\item Otherwise, if $X\cap body^-(r)\neq\emptyset$, then we delete the whole rule.
\item We replace every constraint $ :- body$ by $\perp :- body$. Here $\perp$ is an atom that does not appear in $P$. As a consequence, $\perp$ does not belong to any Answer Set.
\item For every rule $r$ with an aggregate in the head: $l\{a_1, a_2, ..., a_p\}u \leftarrow b_1, b_2, ..., b_m$ (we suppose that we have already removed the negative literals according to the first method)
\begin{itemize}
\item if $l \leq |\{a_1, a_2, ..., a_p\}\cap X | \leq u$, we replace $r$ by all the rules in the following set: $\{ a_i \leftarrow b_1, b_2, ..., b_m | a_i \in X\}$.
\item otherwise, we replace $r$ by $\perp \leftarrow b_1, b_2, ..., b_m$
\end{itemize}
\end{itemize}

Thus, the reduct $P^X$ is a \textit{definite} logic program (which means that it does not contain any negation as failure). Definite logic programs have a unique minimal Herbrand model, that is very easy to construct. We will write $M(P^X)$ the minimal Herbrand model of $P_X$.

\smallskip

We call \textit{stable model} of $P$ every Herbrand interpretation $X$ that satisfies the relation: $X=M(P^X)$. Moreover, as we do not use classical negation $\neg$, we will not make a distinction between \textit{answer sets} and stable models.

\smallskip

Stable models of $P$ have more properties than its models in general: they also are \textit{minimal} (for inclusion) and \textit{supported} (every atom in the stable model appears in a rule whose body evaluates to \textit{true}). 

\subsection{Tools}

For ASP problems, we will use some of the tools developed by the University of Potsdam: \texttt{gringo}, \texttt{clasp} and \texttt{clingo}. \texttt{gringo} transforms the logic program in input into an equivalent variable-free program (it is a \textit{grounder}). And \texttt{clasp} is a \textit{solver} capable of solving ground logic programs. Finally, \texttt{clingo} only combines grounding (with \texttt{gringo}) and solving (with \texttt{clasp}).

\smallskip

We used the version 4.3.0 of \texttt{clingo} since this the one that ILASP uses.
%TODO : reference manuel

\subsection{\colorbox{yellow}{Optimization in clingo}}

In \texttt{clingo}, it is possible to explain what kind of answer sets are optimal by using \textit{weak constraints}. A weak constraint has the following form:
\begin{center}
\texttt{:$\sim$ t$_1$, t$_2$, t$_3$,..., t$_n$.[w@p, id$_1$, id$_2$, ..., id$_m$]}
\end{center}
where \texttt{w} is called the \textit{weight}, and \texttt{p} is called the \textit{priority} of the weak constraint. We will also call \textit{ID} of the weak constraint the list \texttt{[\texttt{w}, \texttt{p}, id$_1$, id$_2$, ..., id$_m$]}. We say that a weak constraint is satisfied by an answer set $A$ if there is at least one atom among t$_1$, t$_2$, t$_3$,..., and t$_n$ that is not in $A$.

\smallskip

We call cost of an answer set $A$ for a given priority \texttt{p} (written $cost_{\texttt{p}}(A)$) the sum of the weights \texttt{w} of the different lists \texttt{[\texttt{w}, \texttt{p}, id$_1$, id$_2$, ..., id$_m$]} such that: there is at least a weak constraint whose ID is  \texttt{[\texttt{w}, \texttt{p}, id$_1$, id$_2$, ..., id$_m$]} that is not satisfied by $A$.

\bigskip

As a consequence:
\begin{itemize}
\item if we prefer the answer sets that satisfy a constraint \texttt{:- t$_1$, t$_2$, t$_3$,..., t$_n$.}, then we can use a weak constraint of the form \texttt{:$\sim$ t$_1$, t$_2$, t$_3$,..., t$_n$.[w@p, id$_1$, id$_2$, ..., id$_m$]} where the weight \texttt{w} is \textbf{positive}.
\item if we prefer the answer sets that satisfy all the atoms \texttt{t$_1$, t$_2$, t$_3$,...,} and \texttt{t$_n$} at the same time, then we can use a weak constraint of the form \texttt{:$\sim$ t$_1$, t$_2$, t$_3$,..., t$_n$.[w@p, id$_1$, id$_2$, ..., id$_m$]} where the weight \texttt{w} is \textbf{negative}.
\end{itemize}

\smallskip

For \texttt{clingo}, in a given ASP program $P$, an answer set $A_1$ \textbf{is preferred to} $A_2$ if and only if $cost_{\texttt{p}}(A_1)<cost_{\texttt{p}}(A_2)$ where \texttt{p} is the lowest priority for which $cost_{\texttt{p}}(A_1)\neq cost_{\texttt{p}}(A_2)$.

\begin{example}
We consider the following ASP program:\newline
\texttt{1\{go\_on\_holidays ; work\_at\_imperial\}1.\\
bad\_mark :- go\_on\_holidays.\\
good\_mark :- work\_at\_imperial.}

\smallskip

To this program we add the two following weak constraints:\newline
\texttt{:$\sim$ go\_on\_holidays.[-1@1] \% we enjoy holidays (priority 1)\\
:$\sim$ good\_mark.[-1@2] \% we like good marks (priority 2)}

\smallskip

This ASP program has two answer sets: $A_1 = \{ \texttt{go\_on\_holidays}, \texttt{bad\_mark}\}$ and $A_2 = \{ \texttt{work\_at\_imperial}, \texttt{good\_mark} \}$. 

\begin{itemize}
\item $A_1$ satisfies only the second weak constraint. So its scores are $cost_1(A_1)=-1$ and $cost_2(A_1)=0$.
\item $A_2$ satisfies only the first weak constraint. So its scores are $cost_1(A_2)=0$ and $cost_2(A_2)=-1$.
\end{itemize}

The highest priority is 2 and $cost_2(A_2)<cost_2(A_1)$. Thus, $A_2=\{ \texttt{work\_at\_imperial}, \texttt{good\_mark} \}$ is optimal.

\end{example}


%TODO : référence manuel ou référence à l'article évoqué dans manuel.

\section{Single-Player Games in ASP}

It is possible to formalize and solve Single-Player Games in ASP. To illustrate the predicates that can be used to describe these games, we will focus on a basic example: a simple graph game.

%TODO : référence 

\subsection{Rules of the game}

The rules of the graph game on figure \ref{fig:agent} are the following:
\begin{itemize}
\item The player starts in state $a$.
\item If the player is in state $a$ or in state $b$ then he can go to the left or to the right.
\item The player wins iff he goes to the state \textit{"victory"}.
\item The player cannot go back to state $a$ once he has reached the state \textit{"hole"}.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\hsize]{figures/diagram1.png}
\caption{Simple graph game}
\label{fig:agent}
\end{figure}

\subsection{Formalizing these rules}

\begin{itemize}



\item We start by defining the different players in the game by using the predicate \texttt{role}. Here, there is only one player so we include the fact: \texttt{role(player).}

\smallskip

\item Then, we define what are the initial states by using the predicate \texttt{holds} for the first time-step:\newline
\texttt{holds(hole, empty, 1).}\\
\texttt{holds(a, player, 1).} \% the player is in state $a$\\
\texttt{holds(b, empty, 1).}\\
\texttt{holds(victory, empty, 1).}

\item We also say what actions are legal at time $T$, depending on the state the player is:\newline
\texttt{legal(player,go(left),T) :- holds(a,player,T) ; holds(b,player, T)}\\
\texttt{legal(player,go(right),T) :- holds(a,player,T) ; holds(b,player,T)}

\item And we explain the situation at time $T+1$ depends on what holds at time $T$ and what the player does:\newline
\texttt{holds(hole,player,T+1) :- holds(a,player,T), does(player,go(left),T).}\\
\texttt{holds(b,player,T+1) :- holds(a,player,T), does(player,go(right),T).}\\
\texttt{...}

\item We can also define what is a \texttt{terminal} state, and we can evaluate each final state by using the predicate \texttt{goal}: \newline
\texttt{terminal(T) :- holds(victory, player, T).}\\
\texttt{terminal(T) :- holds(hole, player, T).}\\
\texttt{goal(player,100,T) :- holds(victory, player, T).}\\
\texttt{goal(player,  0,T) :- holds(hole, player, T).}
\end{itemize}

We will follow the following Game Description Language (GDL) restriction: the predicate \texttt{does} does not appear in the definition of \texttt{terminal}, \texttt{goal} or \texttt{legal}.

\subsection{Finding a solution to the game}

We have just defined a few predicates to translate the rules of the game in ASP. Now we want to simulate the game and to find winning sequences of moves.
\begin{itemize}
\item First of all, the player can do only one move at every time step (as long as the game is not finished).\newline
\texttt{1\{does(player,go(left),T);does(player,go(right),T)\}1 :- not terminated(T).}\\
Where \texttt{terminated(T)} is true if and only if there is a T1 such that $T1<T$ and \texttt{terminal(T1)} is true:\newline
\texttt{terminated(T) :- terminal(T).}  \\
\texttt{terminated(T+1) :- terminated(T).}

\item Also, every move that is played must be legal:\newline
\texttt{:- does(player, M, T), not legal(player, M, T).}

\item We want the game to terminate at some point:\newline
\texttt{:- 0\{terminated(T) : time\_domain(T)\}0.}\\
\texttt{time\_domain(1..10). \% We allow at most 10 moves} 

\item Finally, we would like to find a sequence of moves in order to win the game:\newline
\texttt{:- terminal(T), not goal(player, 100, T)}

\end{itemize}

\section{Inductive Logic Programming}

In this part, $B$ is the logic program that represents the \textit{Background Knowledge}, $S_M$ is the set of possible hypotheses, $E^+$ is the set of positive examples (all the elements that have been observed) and $E^-$ is the set of negative examples (all the elements that can't appear in any result). Our task is to find an hypothesis $H\in S_M$ such that $B\cup H$ \textit{explains} $E^+$ and $E^-$. There are different ways of defining the word "explains" used in the last sentence. We will have a look at three of them.

%TODO S_M ???

\subsection{Brave Inductive Logic Programming}

\subsubsection{Brave Induction}

A brave inductive task is a tuple $T_b=<B, E^+, E^->$ (where $E^+$ and $E^-$ are sets of atoms occurring in $B$) such that: an hypothesis $H$ is solution of $T_b$ iff there is an answer set $A$ of $B\cup H$ verifying $E^+\subseteq A$ and $E^-\cap A = \emptyset$

\smallskip

We call $ILP_b(B,E^+,E^-)$ the set of hypotheses that are solutions of $T_b$. 

\subsubsection{ASPAL encoding}

By using ASPAL, we can find the optimal solutions to a brave inductive task. We
illustrate how to use the ASPAL encoding with clingo in the following example.\bigskip

First of all, we consider the background knowledge B:\newline
\texttt{person(Nicolas). person(Pierre).\\
play\_video games(Nicolas). play\_video\_games(Pierre). works(Nicolas).}\\

And we take: $E^+=\{\texttt{good\_marks(Nicolas)}\}$ and $E^-=\{\texttt{good\_marks(Pierre)}\}$

\begin{itemize}
\item We are studying the hypotheses that are following this mode: \texttt{modeh(good\_marks(+X))}, \texttt{modeb(1,works(+X))} and \texttt{modeb(1,play\_video\_games(+X))}. So we write all the possible hypotheses, and we add the predicate rule/1 that takes as an argument the id of the hypothesis.\newline
\texttt{good marks(P) :- rule(1).\\
good\_marks(P) :- works(P), rule(2).\\
good\_marks(P) :- play\_video\_games(+P), rule(3).\\
good\_marks(P) :- works(P), play\_video\_games(+P), rule(4).}

\item We will select some rules between those above:\newline
\texttt{\{rule(1..4).\}}

\item And we want to respect the examples:\newline
\texttt{goal :- good\_marks(Nicolas), not good\_marks(Pierre).\\
:- not goal.}

\item Finally, we want to find the optimal (the shortest) hypothesis:\newline
\texttt{\#minimize[rule(1)=1,rule(2)=2,rule(3)=2,rule(4)=3].}

\end{itemize}




%TODO défauts d'ASPAL

\subsection{Cautious Inductive Logic Programming}

A cautious inductive task is a tuple $T_c=<B, E^+, E^->$ (where $E^+$ and $E^-$ are sets of atoms occurring in $B$) such that: an hypothesis $H$ is solution of $T_c$ iff 
\begin{itemize}
\item $B\cup H$ has at least one answer set
\item every answer set of $B\cup H$ verifies $E^+\subseteq A$ and $E^-\cap A = \emptyset$
\end{itemize}

We call $ILP_c(B,E^+,E^-)$ the set of hypotheses that are solutions of $T_c$. 

%TODO pas possible avec clingo

\subsection{Inductive Learning From Answer Sets Programming}

We present here a new type of inductive task which is more general than the cautious and the brave inductive tasks.

\subsubsection{Partial Interpretations}

We call \textit{partial interpretation} a tuple $<e^+,e^->$ where $e^+=\{e^+_1,e^+_2,...,e^+_m\}$ and $e^-=\{e^-_1,e^-_2,...,e^-_n\}$ are two sets of atoms.

\smallskip

We say that an interpretation $I$ of $B$ extends a partial interpretation $<e^+,e^->$ iff $e^+ \subseteq I$ and $e^- \cap I = \emptyset$.

\subsubsection{Learning from Answer Sets Induction}

\begin{definition}

A \textit{Learning from Answer Sets task} is a tuple $T_{LAS}=<B, S_M, E^+, E^->$ (where $E^+$ and $E^-$ are sets of partial interpretations) such that: a hypothesis $H$ is solution of $T_{LAS}$ iff 
\begin{itemize}
\item For all  $<e^+,e^->\in E^+$, there is an answer set of $B\cup H$ that extends $<e^+,e^->$
\item For all  $<e^+,e^->\in E^-$, there are no answer sets of $B\cup H$ that extend $<e^+,e^->$
\end{itemize}

We call $ILP_{LAS}(B,S_M,E^+,E^-)$ the set of hypotheses that are solutions of $T_{LAS}$. 
\end{definition}

\begin{example}

\end{example}

\subsubsection{Learning from Ordered Answer Sets}

The definition of Learning from Answer Sets task can be extended to take into account the ordering relations between the different answer sets of the ASP program. We will first define what is an ordering example, and how it can be used to represent the ordering relations \citep{law2015weak}.

\begin{definition}

We will call \textit{ordering example} every pair $<e_1,e_2>$ of partial interpretations. We define two ways for an ASP program to cover an ordering example:
\begin{itemize}
\item An ordering examples is \textit{bravely covered} by an ASP program $P$ iff \textbf{there are} two answer sets $A_1$ and $A_2$ such that $A_1$ extends $e_1$, $A_2$ extends $e_2$ and $A_1$ is preferred to $A_2$.
\item An ordering example is \textit{cautiously covered} by an ASP program $P$ iff \textbf{for every} answer sets $A_1$ and $A_2$ such that $A_1$ extends $e_1$, $A_2$ extends $e_2$ and $A_1$ is preferred to $A_2$.
\end{itemize}
\end{definition}

Now we can give the definition for a Learning from Ordered Answer Sets task \citep{law2015weak}:

\begin{definition}
A \textit{Learning from Ordered Answer Sets task} is a tuple $T_{LOAS}=<B, S_M, E^+, E^-,O^{brave}, O^{cautious}>$ (where $E^+$ and $E^-$ are sets of partial interpretations and $O^{brave}$ and $O^{cautious}$ are sets of ordering examples) such that: every partial interpretation that appears in $O^{brave}$ and $O^{cautious}$ is in $E^+$, and a hypothesis $H$ is solution of $T_{LOAS}$ iff 
\begin{itemize}
\item $H$ is a solution of $T_{LAS}=<B, S_M, E^+, E^->$.
\item Every ordering example in $O^{brave}$ is bravely covered by $B\cup H$
\item Every ordering example in $O^{cautious}$ is cautiously covered by $B\cup H$
\end{itemize}

We call $ILP_{LOAS}(B,S_M,E^+,E^-,O^{brave},O^{cautious})$ the set of hypotheses that are solutions of $T_{LOAS}$.
\end{definition}

\begin{example}

\end{example}

\subsubsection{Learning from Context Dependant Answer Sets}
%TODO : talk about ILASP --2i

The two tasks defined above can be generalized by adding a \textit{context} to every partial interpretation \citep{law2016context}. A \textit{context} is an ASP program that does not contain any weak constraint or any other statement used to classify answer sets. Instead of containing partial interpretations, $E^+$ and $E^-$ contain tuples $<<e^+,e^->,C>$ where $C$ is the \textit{context} of the partial interpretation.  

\begin{definition}

\end{definition}

\begin{example}

\end{example}

\subsubsection{ILASP}

ILASP is a tool developed at Imperial College London that is capable of finding optimal hypotheses for Learning from Answer Sets tasks. We only need to give it: the background knowledge with the \texttt{clingo} syntax, and the mode declaration for the variables that can appear in the head or in the body of a rule in the hypothesis. We can also specify the positive and negative partial interpretations of the task.

\paragraph{Learning rules of games/environment}

ILASP has been used for learning the rules of Sudoku. Also, a simulated agent in an unknown environment could learn progressively the rules of this environment (such that: ”the agent cannot go through a wall”, ”the agent has to get the key before entering a locked cell”...).

\paragraph{Learning weak constraints}

ILASP can also be used to solve Learning from Ordered Answer Sets tasks

%TODO : cite second paper from Mark

\smallskip

Do to so, we specify the partial interpretations that belong to $E^+$ and we give them names (like \texttt{pos1}, \texttt{pos2}...). Then there are two ways to specify the ordering relations between some answer sets:
\begin{itemize}
\item 
\item 
\end{itemize}

\paragraph{Learning from Context Dependant Answer Sets} 

Instead of containing partial interpretations, $E^+$ and $E^-$ contain tuples $<<e^+,e^->,C>$ where $C$ is a \textit{context}. 

\smallskip

We say that an hypothesis $H$ is solution of $T^{Context}_{LAS}=<B,S_M,E^+,E^->$ iff
\begin{itemize}
\item for all $<< e^+ , e^->, C >\in E^+$, there is an Answer Set $A$ of $B \cup H \cup C$ such that $A$ extends $< e^+ , e^- >$


\item for all $<< e^+ , e^->, C >\in E^+$, there are no Answer Set $A$ of $B \cup H \cup C$ such that $A$ extends $< e^+ , e^- >$

\end{itemize}

\paragraph{Learning from Noisy Examples}

ILASP (v3) is also able to learn from noisy data: it can learn an hypothesis that covers a majority (or the most important) examples. To do so, we need to specify which examples are in the noisy data, and we give a weight to them. 

\smallskip

ILASP will minimize the sum of the length of the hypothesis and of the weights of the uncovered examples (the weight of an example reflects its importance).

%TODO : décrire l'algo utilisé par ILASP ? Ou comparaison des différentes version d'ILASP sur le fonctionnement de l'algo
%TODO : 

